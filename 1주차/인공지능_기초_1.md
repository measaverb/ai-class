# 인공지능 기초를 위한 FAQ (Part 1)

1. 인공지능에서 지능에 해당하는 기능은 무엇인가?

2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)

지도학습 (supervised learning): 학습 시 사용하는 데이터 샘플과 대응되는 정답값 (task-wise) 이 존재하고, 정답값과 예측값의 차이를 인풋 - 아웃풋 상관관계 모델링

비지도학습 (unsupervised learning): 학습 시 정답값을 사용하지 않는 기조, 데이터의 차원을 축소하여 clustering을 하거나, 보조 태스크 (pretext task) 를 설계하여 정답값 없이 유의미한 feature를 뽑아낼 수 있도록 하는 self-supervised learning이 포함됨

반지도학습 (semi-supervised learning): 정답값이 있는 데이터 샘플과 정답값이 없는 데이터 샘플을 둘 다 가용하는 기법을 총망라하는 용어. 보통 정답값이 있는 데이터 샘플을 가지고 훈련된 모델을 이용해 정답값이 없는 레이블에 추정 정답값을 넣는 self-labelling 기법이 사용됨

강화학습 (reinforcement learning): 실험자가 미리 환경을 구성하고 에이전트 환경 안에서 여러가지 상호작용을 통해 학습하는 형태. 환경에는 보상과 처벌이라는 요소가 있을 수 있으며, 에이전트는 보상을 최대화하고 처벌을 최소화하는 행동을 하도록 훈련됨

3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?

전통적인 프로그래밍 방법은 특정 태스크를 풀기 위하여 프로그래머가 직접 룰을 세우고 구현한다. - rule based algorithm

인공지능 프로그램은 특정 태스크를 풀기 위하여 프로그램이 데이터 간 상관관계를 모델링한다.

4. 딥러닝과 머신러닝의 차이점은 무엇인가?

딥러닝은 머신러닝 범주 안에 있음. 고전적인 머신러닝 기법과 딥러닝 기법의 차이는 실험자가 feature engineering, feature selection 같은 과정을 가지는 지 유무에 있다.

딥러닝은 레이어를 다층으로 쌓아 feature extraction을 하는데, implicit 하게 데이터 간 상관관계를 잘 찾을 것이라는 가정하에 있다.

5. Classification과 Regression의 주된 차이점은?

Classification은 discrete value를 출력해야 하는 태스크, regressions은 continouous value를 출력해야 하는 태스크

6. 머신러닝에서 차원의 저주(curse of dimensionality)란?

Feature의 dimension을 늘리면 점차 성능이 늘어나지만, 어느 수준에 올라가면 성능이 급격히 떨어지는 현상.

고차원으로 올라가면 올라갈 수록 의미있는 관계를 모델링 하기 위한 데이터 수는 증가하지만, 훈련 시 사용되는 데이터 수는 고정되어 있다. 또한, 고차원으로 가면 갈 수록 거리의 차이가 떨어지므로 clustering 시 유의가 필요함.

7. Dimensionality Reduction는 왜 필요한가?

차원의 저주를 해소하기 위하여 feature의 dimension을 줄인다. 이 때, feature selection, feature engineering 류 기법을 사용할 수 있다.

고차원의 feature를 사용하면 노이즈 양도 비례하여 늘어나므로, dimensionality reduction 시, 데이터 내 noise reduction의 효과 또한 볼 수 있으며, 사용되는 computation cost 를 줄일 수 있다.

8. Ridge와 Lasso의 공통점과 차이점? (Regularization, 규제 , Scaling)

9. Overfitting vs. Underfitting

Underfitting: 훈련이 덜 되어 인풋 - 아웃풋 간 상관관계를 잘 나타내지 못하는 상태

Overfitting: 훈련 데이터를 외운 상태, 훈련 데이터 내에 필연적인 노이즈까지 학습하여 새로운 데이터에 일반화 성능 떨어지는 모습 보임 (degradation on validation/test set)

10. Feature Engineering과 Feature Selection의 차이점은?

11. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)

12. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)

13. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?

단순한 linear regression을 예로 들어, y = ax + b 식이 있을 때 a가 기울기이며, b가 y 절편이다.

각각 의미하는 바는

- 기울기 (a): 인풋 - 아웃풋 간 상관관계의 방향 및 강도에 대한 값
  - a > 0이면 양의 상관관계
  - a < 0이면 음의 상관관계
  - 기울기의 크기는 상관관계 강도
- y 절편 (b): x가 0일 때의 값이므로 시작점이 됨. x가 0일 가능성이 높을 때 유의미할 수 있음.

딥러닝은 퍼셉트론을 여러 층으로 쌓은 것이고, 퍼셉트론은 y = ax + b로 표현할 수 있다.

14. Activation function 함수를 사용하는 이유? Softmax, Sigmoid 함수의 차이는?

비선형성을 주어 복잡한 패턴을 배울 수 있도록 함. Activation function을 사용하지 않는다면 인공신경망은 선형 결합의 연속일 뿐임. 또한, Softmax나 Sigmoid 같은 함수를 이용하여 원하는 값으로 변환할 수 있다.

Sigmoid는 주로 binary classification 시 마지막 dense layer에 사용되는 activation function으로, 입력 feature의 모든 dimension에 독립적으로 0에서 1 사이 값으로 매핑한다.

Softmax는 주로 multi-class classification 시 마지막 dense layer에 사용되는 activation function으로, 입력 feature를 확률 분포 (모든 dimension의 값을 더할 시 1이 되도록) 로 변환한다.

그러나 식을 봤을 때, softmax는 본질적으로 sigmoid를 multi-class에 가용 가능하도록 확장한 actiation function이다.

15. Forward propagation, Backward propagation이란?

Forward propagation: 인공신경망이 입력값을 받아 파라미터를 곱하고 더하며 출력값을 뽑는 일련의 과정

Backward propagation: 인공신경망 내에 있는 각 파라미터에 대하에 대하여 loss function의 기울기를 구하는 과정

16. 손실함수란 무엇인가? 가장 많이 사용하는 손실함수 4가지 종류는?

손실함수란 정답값과 예측값 간 오차를 정량화하는 함수이다. 직관적으로 말하자면, 예측이 얼마나 틀렸는 지에 대한 값으로 이해할 수 있다.

가장 많이 사용하는 손실 함수 4가지:

- Binary Classification - binary cross entropy loss

- Multi-class Classification - categorical cross entropy loss

- Regression - Mean Absolute Error, Mean Squared Error

17. 옵티마이저(optimizer)란 무엇일까? 옵티마이저와 손실함수의 차이점은?

Optimizer의 역할은 가장 적은 loss 값을 낼 수 있는 파라미터를 찾는 것이다. (최적화)

최적화 방식은 각 파라미터 값에 대하여 loss 값의 기울기 (미분 거침, 인공신경망은 여러 노드들이 겹처 있으므로 chain rule 이용)를 계산하고, 기울기 값을 기반으로 파라미터 값을 수정한다.

Loss function은 예측값과 정답값 간 차이이다.

18. 경사하강법 의미는? (확률적 경사하강법, 배치 경사하강법, 미니 배치경사하강법)

optimisation의 한 방법론으로, 각 파라미터에 대하여 loss 의 기울기를 구하고, 기울기의 반대 방향으로 파라미터를 업데이트 하여, loss 값을 최소화한다. update p := p - (lr * pg)

배치 경사하강법, 확률적 경사하강법, 미니배치 경사하강법에 대한 차이는 어떤 단위로 경사하강법 최적화를 할 것인지에 따라 서로 다르다.

배치 경사하강법은 에포크 단위로 경사 하강법을 진행한다.

확률적 경사하강법은 파라미터 업데이트 이터레이션 마다 랜덤으로 데이터 샘플을 정하여 이를 기반으로 경사 하강법을 진행한다.

미니배치 경사하강법은 데이터셋을 여러 작은 단위로 쪼개어 각 샘플마다 경사 하강법을 진행한다.
