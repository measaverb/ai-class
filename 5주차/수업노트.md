# 5주차 수업

## Transformer

### RNN, LSTM의 한계

데이터의 순서가 중요한 시계열 데이터 혹은 자연어 처리에 사용된다

Gradient Vanishing / exploding, 입력과 출력의 길이가 고정됨 -> 보완하기 위한 seq2seq이 제안되었지만 gradient vanishing 문제는 유지

### Attention

특정 데이터 포인트가 다른 데이터 포인트와 얼마나 관련되어 있는지 파악하도록 하는 메커니즘

### Transformer

Encoder와 Decoder로 나누어져 있다.

### Transformer Encoder



