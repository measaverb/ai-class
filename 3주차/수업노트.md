# 3주차 수업

## 이번 주 메인 수업

- 차원 축소 (Dimensionality Reduction)

고차원으로 올라가면 올라갈 수록 의미있는 관계를 모델링 하기 위해 필요한 데이터 수는 증가하지만, 훈련 시 사용되는 데이터 수는 고정되어 있다. 또한, 고차원으로 가면 갈 수록 거리의 차이가 떨어지므로 clustering 시 유의가 필요함. 이 현상으로 인해 별다른 고려 없이 feature dimension을 늘리면 차원의 저주가 일어난다.

차원의 저주를 해결하기 위하여 feature의 dimension을 줄인다. 이 때, feature selection, feature engineering 류 기법을 사용할 수 있다.

고차원의 feature를 사용하면 노이즈 양도 비례하여 늘어나므로, dimensionality reduction 시, 데이터 내 noise reduction의 효과 또한 볼 수 있으며, 사용되는 computation cost 를 줄일 수 있다.

- Manifold Learning

고차원의 데이터를 저차원의 데이터로 만들어 학습하는 기조. 2가지 가정 하에 있다. 1. 고차원의 데이터를 저차원으로 나타낼 수 있다. 2. 저차원으로 줄이게 되면, 데이터를 파악하기 쉽다.

MNIST 데이터셋 내의 데이터 샘플은 $$x \in \mathbb{R}^{64 \times 64}$$로 고차원이다.

- Principle Component Analysis

- 비지도학습

비지도학습의 이유는 정답값이 있는 데이터를 얻기 위해서는 비용이 어려우며, 그 반면에 정답값이 없는 데이터는 비교적 많고, 얻기 쉽기 때문이다.

- 딥러닝에서 기억해야 할 5가지

Loss Function: 예측값이 정답값에 비해 얼마나 틀렸는 지를 알려주는 함수

Optimizer: Loss Function의 값을 최소화하는 역할. 주로 Gradient Descent가 사용됨.

Activation Function: 모델 내에 비선형성을 주기 위해서 사용. Activation Function이 없다면 단순 선형 결합임.

Frontpropagation / Backpropagation

- Residual Learning의 이유

첫번째로, gradient vanishing 문제를 해결하기 위해서이다. 두번째로, 잔차를 approximate 함으로서 학습이 쉬워진다. 세번째로, gradient flow를 늘릴 수 있다.

### Convolutional Neural Networks

기존 multi-layer perceptron을 가지고 이미지를 처리한다고 가정하자. 이미지를 h x w 행렬로 만들고 h x 1 사이즈로 resize하여 인풋을 할 수 있다.

그러나, 이미지에서는 위치 관계 및 local feature가 중요한데, h x 1 사이즈로 resize 하게 되면 유의미한 위치 관계가 사라지게 된다.

이를 보완하기 위하여 Convolutional Neural Networks (CNN)가 고안되었다.

Convolution operation을 사용한다.

CNN을 사용하게 되면 loss function 출력값을 최소화하는 filter를 찾는 것이다. Filter의 개수는 인풋 데이터의 채널 개수와 같다.

추출된 feature에는 hierarchy가 있다.

Pooling의 목적: 1. feature의 사이즈를 줄이면서 대표값을 찾기 위해서 2. Being translation-invariant

### Recurrent Neural Networks

![RNN](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5)

t-1에 들어온 데이터와 t에 들어온 데이터가 시간 상에서 독립적인 관계가 이닐 때, temporal information 을 사용하기 위해여 고안; 데이터의 추세성을 반영

추세성을 반영하기 위하여, 이전 timestamp에 들어온 데이터에서 추출한 feature 중 일부를 현재 timestamp에 들어온 데이터를 처리할 때 사용한다.

그러나, sequence가 길어지면 길어질 수록, gradient vanish 현상이 일어난다. 그러므로, 서로 유의미한 관계가 있는 데이터가 timestamp 상 거리가 많이 떨어지면 떨어질 수록, 신경망이 관계를 모델링하지 못할 가능성이 높아진다.

위 현상을 해결하기 위하여 LSTM 아키텍쳐가 나오게 되었다.

Temporal information을 사용하는 가장 기본적인 형태의 신경망이며, 이를 응용하여 다른 variation이 나오게 되었다. 예를 들어, 시간 역방향, 순방향을 동시에 고려하는 Bi-RNN이 있다.

### Long short-term memory

![LSTM](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/1200px-LSTM_Cell.svg.png)

RNN의 gradient vanish를 해결하기 위해 고안된 아키텍쳐이다.

Short term track (a) 과 long term track (c) 을 분리하였다.

Long term track (cell state) 에 추가적인 신경망을 배치하여 어떤 memory를 다음 timestep에 옮길지 정하는 역할을 한다.

구체적으로, forget gate, input gate, candidate memory, output gate로 구성되어 있다.

- Forget Gate: Sigmoid 함수가 있어, input과 hidden state를 선택적으로 받을 수 있도록 함
- Input Gate:
- Candidate Memory:
- Output Gate: 다음 timestamp에 어떻게 보낼 것인 가를 결정하는 부분

LSTM은 RNN에 비해 모듈이 많으므로, 시간이 오래 걸린다. 이를 해결하기 위하여 GRU 아키텍처가 고안되었다.

### Gated Recurrent Unit

![GRU](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)


### Text Embedding

컴퓨터는 문자 그 대로를 이해할 수 없음. 그러므로 숫자로 바꾸어주어야 함.

이 때 사용되는 기법이 Text Embedding인데 문자 데이터를 숫자로 바꾸어주는 역할을 한다.

Text Embdding에는 여러가지 기법이 있는데, 발전 방향은 어떻게 하면 유의미하게 공간 상에서 단어들을 배치할 수 있는가이다.

연구 예로는 Word2Vec, BERT 등이 있다.
