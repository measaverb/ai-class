# 4주차 수업

## 저번 주 리뷰

- Fully Connected Layer 와 CNN의 차이

Convolution Operation을 이용해 이미지 데이터 내 spatial information을 사용할 수 있다.

- CNN과 RNN의 차이

CNN: 이전 입력이 이후 출력에 영향을 끼치지 않음.

RNN: 이전 입력에 의해 이후 출력에 영향을 끼침.

- Variants of RNN

RNN의 레이어가 깊어지면, vanishing gradient 현상 발생. 시퀀스의 길이가 길 수록, 초기에 들어온 입력에 영향이 줄어듦.

LSTM: 위 문제를 해결하기 위한 수정 아키텍처; Cell State 추가 (forget gate, input gate, candidate memory, output gate)

LSTM도 완벽히 vanishing gradient 문제를 해결하지 못함. 마찬가지로, 시퀀스의 길이가 길면 vanishing gradient 현상 발생.

- Residual Learning

![residual](https://miro.medium.com/v2/resize:fit:868/0*sGlmENAXIZhSqyFZ)

Residual Learning은 h(x) = f(x) + x 로 나타낼 수 있음.

Residual Learning의 이유

1. Vanishing Gradient를 해결하기 위해
2. f(x) - x = h(x) 이므로 h(x)의 approximation이 쉬워짐.

## 이번 주 메인 수업

### Pretraining

Domain Adaptation은 모델이 훈련된 데이터셋에서 도메인이 다른 데이터셋을 추론하려 할 때 해결하기 위한 방법을 연구하는 분야.

보통 transfer learning의 변형으로 일어나며, domain adaptation의 대상이 pre-trained model이다.

Pre-trained model이란 특정 데이터셋으로 훈련된 모델을 의미한다. 실 사용 예로, 훈련하기 전 weight initialisation을 할 때, imagenet pre-trained model을 가져오는 경우가 많다.

또, pre-trained model의 linear layer만 제거하여 타겟 데이터셋에 맞도록 새로운 linear layer를 넣고 재훈련하기도 한다.

### Fine-tuning

Fine-tuning 시 pre-trained model을 사용하는데, naïve training은 weight intialisation을 heuristic 하게 하는 반면, pre-trained model을 사용하게 되면 이미 유의미한 weight를 가져오기 때문에 비교적 적은 훈련으로 좋은 정확도를 낼 수 있다.

### Word Embedding

Encoding의 세 가지: 1. 문자를 숫자로 변환 2. One-hot encoding 3. 단어 임베딩

가장 단순한 단어 임베딩은 미리 매핑된 숫자로 나타낼 수 있다.

단어 임베딩을 고도화하려는 연구에서는 의미가 비슷한 단어들을 공간 상에서 가까이 배치하는 신경망을 만드는 것이다. Bert 류 모델들이 이것에 해당된다.

### Variants of CNN

- 초기 버전; LeNet-5, AlexNet, ...
- GoogLeNet
  - Inception Module: 1x1 convolution, multiple convolutional layers
- VGG
- ResNet: skip connection 이용하여 residual learning
- Xception: depthwise separable convolution layer (spatial-only layer (depth-wise) + 1x1 convolution (point-wise))
  ```python
  dscl = nn.Sequential(
      [
          nn.Conv2d(channel_in=channel_in, channel_out=channel_in, kernel=(3, 3))     # depth-wise
          nn.Conv2d(channel_in=channel_in, channel_out=channel_out, kernel=(1, 1))    # point-wise
      ]
  )
  ```
- SENet: extracted feature의 각 채널 별 중요도를 0~1 사이 값으로 표현하도록 훈련

  - 기작은 squeeze 후에 excitation을 거치며 중요한 feature를 뽑도록 한다.
  - Global Average Pooling -> A Dense Layer to Squeeze -> ReLU -> A Dense Layer to Excitate -> Sigmoid
    ![senet](https://miro.medium.com/v2/resize:fit:1120/1*MI4Kc8RPimJi5YZiPuG8Nw.png)

- DenseNet, ResNeXt, MobileNet, EfficientNet, ...

### Miscellaneous

- Gradient explosion 일어날 때는 gradient clipping 기법을 사용, gradient를 일정 값 이하로 제한.
