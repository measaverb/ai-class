# 6주차 수업

## 저번 주 리뷰

1. Pre-trained net 사용 이유
    - 남이 만들어 놓은 복잡한 모델을 그대로 가져다 쓰기 위해서 (복잡도와 정확도 어느 정도 비례하기 때문)
    - 새로운 데이터 훈련을 위한 좋은 시작점이 되기 때문 (relatively better than random weight initialisation)

2. PCA와 Autoencoder
    - 목적이 비슷함. 방법이 다름; Autoencoder - data compression, PCA - dimensionality reduction
    - Latent space: task-wise feature가 있는 공간
    - Latent vector: latent space 내 벡터
    - Anomaly detection using autoencoder
        - The latent space of a specific autoencoder is well-modelled using normal data while training.
        - Reconstruction error of unseen data will be large.
        - However, it seems the difference between normal and abnormal should be significant.


## 이번 주 메인 수업

### Transformer

- Seq2seq 모델로 설계됨
- 가장 중요한 4가지
    - Attention: 특정 출력을 내기 위하여 입력에 어떤 부분에 가중치를 주어야하는지 결정하는 메커니즘.
    - Multi-head Attention: Attention의 병렬적 배치; 다양한 context를 뽑을 수 있음
    - Embedding: 입력을 숫자로 바꾸는 과정 (word2vec, BERT)
    - Positional Encoding: 상대적인 위치 정보 injection, as the Transformer is permutation-invariant.

