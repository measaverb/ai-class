# 7주차 수업

## 저번 주 리뷰

1. CNN, RNN, LSTM

   - CNN은 인접한 pixel 같은 local 정보를 추출하도록 설계됨. 이미지 분석에 사용
   - RNN은 이전에 들어온 정보를 참조할 수 있음. 시계열 혹은 temporal information 이 필요할 때 사용
   - LSTM은 gradient vanishing 문제를 해결하기 위하여 정보를 전달하는 조건 역할의 여러가지 gate를 추가. 더 많은 모듈이 추가되어 비교적 무거우며, sequence의 길이가 더 길어지면 gradient vanishing 문제 발현

2. Variants of CNN

   - ResNet: Skip connection, 1) alleviating gradient vanishing, 2) enables residual learning
   - SENet: 채널 중요도 (channel attention) 를 0 ~ 1 사이 값으로 출력하는 레이어 추가
   - Inception: 동일한 입력에 서로 다른 CNN 레이어를 적용해 다양한 feature를 사용, 1x1 convolutional layer 사용
   - Depthwise Separable Convolution: depth-wise convolutions (각 채널 별 필터 하나씩) + point-wise convolutions (1x1 convolutional layer), Inception에서는 point-wise -> depth-wise 구조
   - CBAM: feature-wise spatial attention 고려

3. Transformer

   - Embedding: 글자를 숫자로
   - Positional Encoding: As transformers are permutation-invariant, inject position dependency
   - Self-attention: 특정 태스크를 위한 출력을 뽑아내기 위해 어떤 입력에 얼마나 집중해야 하는지를 0 ~ 1 사이 값으로 출력하는 기작
   - Multi-head attention: 입력 (query, key, value) 를 헤드 수 만큼 나누어 각각의 attention module에 입력

4. Autoencoder

   - 목적: Dimensionality reduction based on manifold learning (essential information for the task can be extracted when the dimension gets lower)
   - $x$ -> Encoder (outputs latent vector) -> Decoder -> $\hat{x}$ (reconstructed x)

## 이번 주 메인 수업

### Batch Normalisation

신경망 레이어를 계속해서 거치면 가중치 계산으로 인해 배치 내 분포가 달라지게 됨. 층을 거칠 때마다 생길 가능성이 있으며 이 문제를 내부 공변량 변화 (internal covariant shift) 라고 한다.

Batch normalisation에서는 배치 내 평균과 분산을 구해 평균이 0, 분산 1이 되도록 만든다. 이 때, 배치 내 데이터는 gaussian distribution을 따른다는 가정 하에 있다.

### Layer Normalisation

배치 내의 평균과 분산을 얻는 것은 어렵다. computational load 또한 있다. 단일 배치 내에 데이터 샘플 개수에 따라서도 영향을 받는다. Layer normalisation에서는 애초에 레이어 단에서 평균과 분산이 유지되도록 한다.

### Vision Transformer

Transformer의 구조를 image classification 에 적용한 것. 이미지를 여러 토큰으로 쪼개서 시퀀스 형태로 변형한다.

CNN과 달리 vision transformer는 inductive bias가 없기 때문에 데이터가 extensive 하게 필요하다.

inductive bias란 모델 아키텍처 구조로 인해 생기는 출력에 생기는 편향을 의미한다. 예를 들어, CNN 같은 경우에는 convolution 연산을 사용하므로 공간 (local feature) 에 대한 inductive bias가 있다.

그러나, transformer의 attention은 기본적으로 permutation-invariant 하기 때문에 inductive bias가 비교적 적다. inductive bias는 훈련하는 것에 cue가 되기도 하므로, inductive bias가 없는 transformer는 더 많은 훈련 데이터가 필요하다.

### Autoencoder

Latent vector를 생성해내는 encoder
