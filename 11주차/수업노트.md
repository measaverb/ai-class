# 전체 리뷰

- Self-supervised learning

- Semi-supervised learning

- Masking과 noise의 이유
  
  - 추론 시, 정제된 데이터가 들어오지 않을 수 있다. 그러므로, 노이즈 같은 것에 robust 하도록 훈련 시에 주입한다.

- Feature selection
  
  - 특정 task를 수행하는 것에 essential한 feature를 선택하는 것
  - 있는 feature 중에서만 선택한다.

- Overfitting & Underfitting
  
  - 왜 발생하고, 해결법은 무엇인가?
  - Overfitting
    - 발생 이유:
    - 해결 방법: 모델 단순화, regularisation, 데이터 증량, batchnorm, ...
  - Underfitting
    - 발생 이유: 훈련이 덜 되었기 때문에, 데이터가 적어서
    - 해결 방법: 더 훈련, data augmentation

- Embedding
  
  - 딥러닝 모델은 숫자만 입력으로 받을 수 있음. 숫자가 아닌 입력을 숫자로 변환하는 과정
  - 예시: one-hot encoding, word2vec

- Ensemble
  
  - 다른 하이퍼파리미터를 가진 여러 개의 모델을 동시에 훈련하여 추론에 사용하는 기법
  - 대표적인 예시로 random forest 가 있다.
  - bagging (bootstraping & aggregation): 데이터 샘플링에 중복을 허용하여 여러 개의 데이터 스플릿을 만드는 것 <-> boosting

- Hyperparameter tuning
  
  - 성능을 올리기 위하여 훈련을 위한 파라미터를 조정하는 행위

- Confusion matrix
  
  - holistic accuracy 만으로 알 수 없는 모델의 평가를 위하여
  - 특정 클래스에 대하여 어떻게 틀리고, 어떻게 맞추는 지 알 수 있음
  - overfitting, imbalanced dataset 시에 유용하게 쓰임
* F1 Score
  
  - 데이터 불균형이 있을 시 accuracy 지표로는 모델 평가가 엄밀하지 않음
  - Precision: TP / TP + FP
  - Recall: TP / TP + FN
  - F1 Score: 2 _ Precision _ Recall / (Precision + Recall)

* 경사하강법
  
  - 각 파라미터에 대하여 loss의 기울기를 구하고 기울기의 반대 방향으로 파라미터를 업데이트 하여, loss 값을 최소화
  - $\hat{p} = p - \lambda \partial p$
  - loss 값을 낮출 수 있는 weight 값을 찾기 위해서 사용
  - Batch Gradient Descent: 에포크 단위로 경사 하강법을 진행
  - Stochastic Gradient Descent: 파라미터 업데이트 이터레이션 마다 랜덤으로 데이터 샘플을 정하여 이를 기반으로 경사 하강법을 진행
  - Mini-batch Gradient Descent: 데이터셋을 여러 작은 단위로 쪼개어 각 샘플마다 경사 하강법을 진행

* Support Vector Machine
  
  - Support Vector: 데이터가 분류되는 경계 (결정 경계) 에 가장 가까운 데이터 포인트, 초평면
  - Support vector machine은 support vector margin이 커지도록 하는 것

* Gini index/impurity
  
  - Gini index/impurity: 특정 데이터 군집 안에서의 데이터 포인트가 같은 클래스로 구성되어 있는 정도, impurity를 1로 올리는 것이

* Curse of dimensionality
  
  - feature의 차원이 너무 많아 모델이 의미 없는 feature에 방해받아 성능이 저하되는 현상
  - Dimension을 줄이기 위해 principle component analysis를 통해 여러 개의 feature 중 주성분을 찾는다

* Manifold learning and its hypothesis
  
  - 고차원의 데이터를 저차원의 데이터로 만들어 학습하는 기조
  
  - Assumptions
    
    - 고차원의 데이터를 저차원으로 나타낼 수 있다.
    - 저차원으로 줄이게 되면, 데이터를 파악하기 쉽다.
  
  - Representation learning의 근간이 됨

* Activation Function
  
  - 레이어의 조합은 선형 결합의 연속이므로 activation function을 통해 비선형성 주입

* Gradient Vanishing and Gradient Explode
  
  - 발생 이유 및 해결 방법

* Pre-trained net
  
  - naïve training은 weight intialisation을 heuristic 하게 하는 반면, pre-trained net을 사용하게 되면 이미 유의미한 weight를 가져오기 때문에 비교적 적은 훈련으로 좋은 정확도를 낼 수 있다.
  - 좋은 starting point로서 사용될 수 있음

* Convolutional Neural Networks
  
  - Convolution 연산을 이용해 local context (인접한 픽셀 데이터) 정보를 사용할 수 있는 신경망
  
  - Pooling: 대표값 추출 및 사이즈 축소, invariant to object position
  
  - ResNet:
    
    - vanishing gradient를 해결하기 위한 skip connection 추가, residual learning 수행
    - residual learning: 입력과 레이어의 출력 간 차이를 이용하여 학습을 용이하게 하는 방법
    - h(x) = f(x) + x
  
  - Inception:
    
    - 동일한 입력에 대하여 스펙이 다른 필터를 붙여 사용
    - 1x1 convolution: 정보 축약의 역할
    - Depthwise Separable Convolutions 처음 제시 (point-wise (1x1) -> depth-wise)
  
  - Depthwise Separable Convolutions
    
    - Depth-wise + Point-wise
    
    - Depth-wise
      
      - 채널 별 중요도를 확인하고자 하는 목적에 있음
      - 필터 개수 = 채널 개수
    
    - Point-wise
      
      - 픽셀 별 중요도를 확인하고자 하는 목적에 있음
      - 1x1 convolutions
- Recurrent Neural Networks
  
  - ![RNN](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png?74e25518f882f8758439bcb3637715e5)
  
  - t-1에 들어온 데이터와 t에 들어온 데이터가 시간 상에서 독립적인 관계가 이닐 때, temporal information 을 사용하기 위해여 고안; 데이터의 추세성을 반영
  
  - 추세성을 반영하기 위하여, 이전 timestamp에 들어온 데이터에서 추출한 feature 중 일부를 현재 timestamp에 들어온 데이터를 처리할 때 사용한다.
  
  - 그러나, sequence가 길어지면 길어질 수록, gradient vanish 현상이 일어난다. 그러므로, 서로 유의미한 관계가 있는 데이터가 timestamp 상 거리가 많이 떨어지면 떨어질 수록, 신경망이 관계를 모델링하지 못할 가능성이 높아진다.
  
  - 위 현상을 해결하기 위하여 LSTM 아키텍쳐가 나오게 되었다.
  
  - Temporal information을 사용하는 가장 기본적인 형태의 신경망이며, 이를 응용하여 다른 variation이 나오게 되었다. 예를 들어, 시간 역방향, 순방향을 동시에 고려하는 Bi-RNN이 있다.
* Long short-term memory
  
  - ![LSTM](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/1200px-LSTM_Cell.svg.png)
  
  - RNN의 gradient vanish를 해결하기 위해 고안된 아키텍쳐이다.
  
  - Short term track (a) 과 long term track (c) 을 분리하였다.
  
  - Long term track (cell state) 에 추가적인 신경망을 배치하여 어떤 memory를 다음 timestep에 옮길지 정하는 역할을 한다.
  
  - 구체적으로, forget gate, input gate, candidate memory, output gate로 구성되어 있다.
    
    - Forget Gate: Sigmoid 함수가 있어, input과 hidden state를 선택적으로 받을 수 있도록 함
    
    - Input Gate:
    
    - Candidate Memory:
    
    - Output Gate: 다음 timestamp에 어떻게 보낼 것인 가를 결정하는 부분
  
  - LSTM은 RNN에 비해 모듈이 많으므로, 시간이 오래 걸린다. 이를 해결하기 위하여 GRU 아키텍처가 고안되었다.
- Transformers
  
  - Attention: 태스크에 얼맞는 출력을 뽑아내기 위해 어떤 입력에 집중해야 하는지 0과 1 사이의 값으로 나타내는 모듈
  
  - Scaled Dot Product Attention: magnitude가 너무 큰 벡터의 영향을 줄이기 위하여 scaling
  
  - Vision Transformer
  
  - 
