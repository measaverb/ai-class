# 9주차 수업

## 생성형 모델이란

생성형 모델이란 확률적 모델 중 주어진 데이터셋과 비슷한 데이터 포인트를 생성할 수 있는 모델을 이른다. 데이터의 분포를 학습하고, 학습된 분포를 기반으로 새로운 샘플을 만들어낸다. 생성형 모델에는 Diffusion Model, VAEs, 그리고 GANs 등이 있다. 구체적으로, 훈련 시, 입력 데이터와 latent vector, variable 간 joint probability distribution을 학습한다.

## Variational Autoencoders

Variational Autoencdoers는 생성형 모델의 일종으로, 데이터를 latent space로 인코딩하고, 다시 원래의 공간으로 디코딩한다. 이전의 autoencoders와 다르게, latent space에서의 확률 분포를 뽑아낸다.

### 구성

- **Encoder**: Maps the input data to a latent space, producing a mean and variance for each latent variable.
- **Latent Space**: A lower-dimensional space where the data is represented in a compressed form.
- **Decoder**: Reconstructs the data from the latent space representation.
- **Reparameterisation Trick**: Ensures that the model can be trained using gradient descent by allowing backpropagation through the stochastic sampling process.

### Loss Function

Reconstruction terms와 regularisation term의 조합으로 이루어져 있다.

1. **Reconstruction Term**: Measures how well the decoder reconstructs the input data. e.g., MSE, MAE, ...
2. **Regularisation Term**: Regularises the learned latent space distribution is close to a prior distribution (usually a standard normal distribution). Using Kullback-Leibler Divergence

### Likelihood

probability: P(data|distribution)

likelihood: P(distribution|data)

Likelihood refers to the probability of the observed data given a set of parameters. Specifically, it measures how likely the observed data is under the model's assumptions. The goal during training is to maximize this likelihood, which involves optimizing the parameters of the encoder and decoder to best explain the observed data. This is often done by maximizing the Evidence Lower Bound (ELBO), which provides a tractable approximation to the true likelihood.

The ELBO consists of two main components:

1. **Reconstruction Likelihood**: This term measures how well the decoder can reconstruct the input data from the latent representation.
2. **KL Divergence**: This term ensures that the learned latent space distribution is close to the prior distribution, promoting regularization.

By maximizing the ELBO, VAEs effectively balance the trade-off between accurately reconstructing the input data and maintaining a well-structured latent space.

## Generative Adversarial Networks
